Q_model:
  custom_model: null
  custom_model_config: {}
  fcnet_activation: relu
  fcnet_hiddens:
  - 256
  - 256
  post_fcnet_activation: null
  post_fcnet_hiddens: []
_deterministic_loss: false
_disable_action_flattening: false
_disable_execution_plan_api: true
_disable_preprocessor_api: false
_fake_gpus: false
_tf_policy_handles_more_than_one_loss: false
_use_beta_distribution: false
action_space: null
actions_in_input_normalized: false
always_attach_evaluation_results: false
batch_mode: truncate_episodes
buffer_size: -1
clip_actions: false
clip_rewards: null
collect_metrics_timeout: -1
compress_observations: false
create_env_on_driver: false
custom_eval_function: null
custom_resources_per_worker: {}
disable_env_checking: false
eager_max_retraces: 20
eager_tracing: false
env: null
env_config: {}
env_task_fn: null
evaluation_config: {}
evaluation_duration: 10
evaluation_duration_unit: episodes
evaluation_interval: null
evaluation_num_episodes: -1
evaluation_num_workers: 0
evaluation_parallel_to_training: false
exploration_config:
  type: StochasticSampling
explore: true
extra_python_environs_for_driver: {}
extra_python_environs_for_worker: {}
fake_sampler: false
framework: tf
gamma: 0.99
grad_clip: null
horizon: null
ignore_worker_failures: false
in_evaluation: false
initial_alpha: 1.0
input: sampler
input_config: {}
input_evaluation:
- is
- wis
keep_per_episode_custom_metrics: false
learning_starts: 1500
local_tf_session_args:
  inter_op_parallelism_threads: 8
  intra_op_parallelism_threads: 8
log_level: WARN
log_sys_usage: true
logger_config: null
lr: 0.0001
metrics_episode_collection_timeout_s: 180
metrics_num_episodes_for_smoothing: 100
metrics_smoothing_episodes: -1
min_iter_time_s: -1
min_sample_timesteps_per_reporting: null
min_time_s_per_reporting: 1
min_train_timesteps_per_reporting: null
model:
  _disable_action_flattening: false
  _disable_preprocessor_api: false
  _time_major: false
  _use_default_native_models: false
  attention_dim: 64
  attention_head_dim: 32
  attention_init_gru_gate_bias: 2.0
  attention_memory_inference: 50
  attention_memory_training: 50
  attention_num_heads: 1
  attention_num_transformer_units: 1
  attention_position_wise_mlp_dim: 32
  attention_use_n_prev_actions: 0
  attention_use_n_prev_rewards: 0
  conv_activation: relu
  conv_filters: null
  custom_action_dist: null
  custom_model: null
  custom_model_config: {}
  custom_preprocessor: null
  dim: 84
  fcnet_activation: tanh
  fcnet_hiddens:
  - 256
  - 256
  framestack: true
  free_log_std: false
  grayscale: false
  lstm_cell_size: 256
  lstm_use_prev_action: false
  lstm_use_prev_action_reward: -1
  lstm_use_prev_reward: false
  max_seq_len: 20
  no_final_linear: false
  post_fcnet_activation: relu
  post_fcnet_hiddens: []
  use_attention: false
  use_lstm: false
  vf_share_layers: true
  zero_mean: true
monitor: -1
multiagent:
  count_steps_by: env_steps
  observation_fn: null
  policies: {}
  policies_to_train: null
  policy_map_cache: null
  policy_map_capacity: 100
  policy_mapping_fn: null
  replay_mode: independent
n_step: 1
no_done_at_end: false
normalize_actions: true
num_cpus_for_driver: 1
num_cpus_per_worker: 1
num_envs_per_worker: 1
num_gpus: 0
num_gpus_per_worker: 0
num_workers: 1
observation_filter: NoFilter
observation_space: null
optimization:
  actor_learning_rate: 0.0003
  critic_learning_rate: 0.0003
  entropy_learning_rate: 0.0003
optimizer: {}
output: null
output_compress_columns:
- obs
- new_obs
output_config: {}
output_max_file_size: 67108864
placement_strategy: PACK
policy_model:
  custom_model: null
  custom_model_config: {}
  fcnet_activation: relu
  fcnet_hiddens:
  - 256
  - 256
  post_fcnet_activation: null
  post_fcnet_hiddens: []
postprocess_inputs: false
preprocessor_pref: deepmind
prioritized_replay: false
prioritized_replay_alpha: 0.6
prioritized_replay_beta: 0.4
prioritized_replay_eps: 1.0e-06
record_env: false
recreate_failed_workers: false
remote_env_batch_wait_ms: 0
remote_worker_envs: false
render_env: false
replay_buffer_config:
  _enable_replay_buffer_api: false
  capacity: 1000000
  type: MultiAgentReplayBuffer
rollout_fragment_length: 1
sample_async: false
seed: null
shuffle_buffer_size: 0
simple_optimizer: -1
soft_horizon: false
store_buffer_in_checkpoints: false
synchronize_filters: true
target_entropy: auto
target_network_update_freq: 0
tau: 0.005
tf_session_args:
  allow_soft_placement: true
  device_count:
    CPU: 1
  gpu_options:
    allow_growth: true
  inter_op_parallelism_threads: 2
  intra_op_parallelism_threads: 2
  log_device_placement: false
timesteps_per_iteration: 100
train_batch_size: 256
training_intensity: null
twin_q: true
use_state_preprocessor: -1
worker_side_prioritization: false
